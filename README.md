# Svd-Evd

\documentclass{report}
\usepackage{xepersian}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\settextfont[Scale=1.2]{BNazanin.ttf}
\setlatintextfont{Times New Roman}



\def\abjad#1{\expandafter\@abjad\csname c@#1\endcsname} 
\def\@abjad#1{\beginR\@@abjad#1\endR}
\def\@@abjad#1{%
\ifcase#1\or الف\or ب\or ج\or د\or ه\or و\or ز\or ح\or ط\or ی\or ک\or ل\or م\or ن\else\@ctrerr\fi}

\newcounter{alef}
\newenvironment{abcd}{
 \setcounter{alef}{0}
 \begin{list} {\numberbox{\abjad{alef}}}
 {\usecounter{alef}}}
 {\end{list}}



\begin{document}

% صفحه عنوان
\begin{titlepage}
\begin{center}
\vspace*{1cm}
\Huge \textbf{گزارش پروژه: کاهش ابعاد داده با استفاده از \lr{SVD} و \lr{EVD}} \\
\vspace{0.5cm}
\LARGE درس: هوش مصنوعی و کارگاه \\
\vspace{0.5cm}
\Large دانشگاه صنعتی امیرکبیر \\
دانشکده ریاضی و علوم کامپیوتر \\
\vspace{1.5cm}
\Large نام دانشجو: مهرداد صالحی \\
شماره دانشجویی: ۴۰۰۱۳۰۲۰ \\
\vspace{1cm}
\Large  استاد: دکتر دهقان \\
\vspace{2cm}
\Large \today
\end{center}
\end{titlepage}

% چکیده
\begin{abstract}
در این پروژه، تاثیر روش‌های کاهش ابعاد داده شامل تجزیه مقادیر منفرد (\lr{SVD}) و تجزیه مقادیر ویژه (\lr{EVD}) بر عملکرد مدل‌های یادگیری ماشین بررسی شد. پنج دیتاست شامل \lr{Wine}، \lr{Sensorless Drive Diagnosis}، \lr{MNIST}، \lr{20 Newsgroups} و \lr{Covertype} انتخاب شدند. داده‌ها با حفظ 95\% و 85\% واریانس کاهش یافتند و مدل‌های \lr{KNN} برای \lr{Wine} و شبکه عصبی برای سایر دیتاست‌ها آموزش داده شدند. نتایج نشان داد که کاهش ابعاد زمان آموزش را به‌ویژه در دیتاست‌های با ابعاد بالا مانند \lr{MNIST} کاهش می‌دهد، در حالی که دقت مدل‌ها در اکثر موارد تنها کمی افت می‌کند. با کاهش واریانس به 85\%، افت دقت بیشتر بود، که نشان‌دهنده اهمیت حفظ واریانس بالاتر است. \\

کلیدواژه‌ها: کاهش ابعاد، \lr{SVD}، \lr{EVD}، یادگیری ماشین، دقت، زمان آموزش
\end{abstract}

% فهرست مطالب
\tableofcontents

% فهرست جداول
\listoftables

% فصل 1: مقدمه
\chapter{مقدمه}
با افزایش حجم داده‌ها در مسائل یادگیری ماشین، دیتاست‌های با ابعاد بالا چالش‌هایی مانند پیچیدگی محاسباتی و خطر بیش‌برازش ایجاد می‌کنند. روش‌های کاهش ابعاد مانند تجزیه مقادیر منفرد (\lr{SVD}) و تجزیه مقادیر ویژه (\lr{EVD}) با حفظ اطلاعات اصلی، تعداد ویژگی‌ها را کاهش می‌دهند و کارایی محاسباتی را بهبود می‌بخشند. این روش‌ها زیرفضایی بهینه از داده‌ها را شناسایی می‌کنند که بیشترین واریانس را حفظ می‌کند.

هدف این پروژه بررسی تاثیر \lr{SVD} و \lr{EVD} بر عملکرد مدل‌های یادگیری ماشین از نظر دقت، زمان آموزش و مصرف منابع محاسباتی بود. برای این منظور، پنج دیتاست با ابعاد مختلف انتخاب شدند و مدل‌های \lr{KNN} و شبکه عصبی روی داده‌های اصلی و کاهش‌یافته آموزش داده شدند. نتایج این مدل‌ها از نظر دقت و زمان آموزش مقایسه شدند تا کارایی این روش‌ها ارزیابی شود.

ساختار این گزارش به شرح زیر است:

* مقدمه: اهمیت کاهش ابعاد و اهداف پروژه

\br
* روش‌شناسی: توضیح دیتاست‌ها، روش‌های کاهش ابعاد و مدل‌های یادگیری ماشین


* نتایج: ارائه نتایج کاهش ابعاد و عملکرد مدل‌ها


* بحث: تحلیل نتایج و مقایسه روش‌ها


* نتیجه‌گیری: جمع‌بندی یافته‌ها و پیشنهادات برای آینده

% فصل 2: روش‌شناسی
\chapter{روش‌شناسی}
\section{داده‌های استفاده‌شده}
پنج دیتاست با ویژگی‌ها و ابعاد مختلف در این پروژه استفاده شدند:

1 \textbf{\lr{Wine}}: شامل 13 ویژگی و 178 نمونه، برای دسته‌بندی به سه کلاس (از \lr{scikit\ndash learn}).


2 \textbf{\lr{Sensorless Drive Diagnosis}}: شامل 48 ویژگی و 58509 نمونه، برای دسته‌بندی به 11 کلاس.


3 \textbf{\lr{MNIST}}: شامل 784 ویژگی (پیکسل‌های تصاویر 28\(\times\)28) و 70000 نمونه، برای دسته‌بندی ارقام دست‌نویس (از \lr{OpenML}).


4 \textbf{\lr{20 Newsgroups}}: دیتاست متنی با 5000 ویژگی پس از تبدیل \lr{TF-IDF}، برای دسته‌بندی به 20 کلاس خبری (از \lr{scikit\ndash learn}).


5 \textbf{\lr{Covertype}}: شامل 54 ویژگی و 581012 نمونه، برای دسته‌بندی نوع پوشش جنگلی (از \lr{scikit\ndash learn}).

تمام دیتاست‌ها با استفاده از \lr{StandardScaler} مقیاس‌بندی شدند و به نسبت 80\% آموزش و 20\% آزمون تقسیم شدند. برای دیتاست \lr{20 Newsgroups}، از \lr{TfidfVectorizer} برای تبدیل داده‌های متنی به بردارهای عددی استفاده شد.

\section{کاهش ابعاد}
دو روش کاهش ابعاد پیاده‌سازی شدند:
\subsection{تجزیه مقادیر منفرد (\lr{SVD})}
\lr{SVD} ماتریس داده \( A \) را به صورت \( A = U \Sigma V^T \) تجزیه می‌کند، که \( U \) و \( V \) ماتریس‌های متعامد و \( \Sigma \) ماتریس قطری مقادیر منفرد است. با انتخاب \( k \) ستون اول \( V \)، داده‌ها به فضای \( k \)-بعدی پروژه شدند. این روش با استفاده از تابع \lr{np.linalg.svd} و پیاده‌سازی دستی (با تجزیه \( A^T A \) و \( A A^T \)) انجام شد.

\subsection{تجزیه مقادیر ویژه (\lr{EVD})}
\lr{EVD} ماتریس کوواریانس \( C = \frac{1}{n} X^T X \) را به صورت \( C = Q \Lambda Q^T \) تجزیه می‌کند، که \( Q \) ماتریس بردارهای ویژه و \( \Lambda \) ماتریس مقادیر ویژه است. با انتخاب \( k \) بردار ویژه اول، داده‌ها به فضای \( k \)-بعدی پروژه شدند. این روش با تابع \lr{np.linalg.eigh} پیاده‌سازی شد.

هدف، حفظ 95\% و 85\% واریانس داده‌ها بود. تعداد ویژگی‌های انتخاب‌شده برای هر دیتاست در بخش نتایج ارائه شده است.

\section{مدل‌های یادگیری ماشین}
دو مدل استفاده شدند:


* \textbf{\lr{KNN}}: برای دیتاست \lr{Wine}، با \( k=5 \)، پیاده‌سازی‌شده با \lr{scikit\ndash learn}.


* \textbf{\lr{شبکه عصبی}}: برای سایر دیتاست‌ها، با سه لایه کاملاً متصل (ورودی، 128 نورون، 64 نورون، خروجی)، فعال‌ساز \lr{ReLU}، تابع هزینه \lr{CrossEntropy} و بهینه‌ساز \lr{Adam}. شبکه برای 10 دوره آموزش داده شد، با استفاده از \lr{PyTorch}.
% فصل 3: نتایج
% فصل 3: نتایج
\chapter{نتایج}
\section{کاهش ابعاد}
جدول \ref{tab:reduction} تعداد ویژگی‌های اصلی و کاهش‌یافته را برای حفظ 95\% و 85\% واریانس نشان می‌دهد:

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
دیتاست & ویژگی‌های اصلی & \lr{SVD} (95\%) & \lr{EVD} (95\%) & \lr{SVD}/\lr{EVD} (85\%) \\
\midrule
\lr{Wine} & 13 & 10 & 10 & 10 \\
\lr{Sensorless} & 48 & 21 & 21 & 14 \\
\lr{MNIST} & 784 & 331 & 331 & 185 \\
\lr{20 Newsgroups} & 5000 & 3172 & 3183 & 2121/2136 \\
\lr{Covertype} & 54 & 43 & 43 & 37 \\
\bottomrule
\end{tabular}
\caption{تعداد ویژگی‌ها قبل و بعد از کاهش ابعاد}
\label{tab:reduction}
\end{table}

\section{عملکرد مدل‌ها}
\subsection{حفظ 95\% واریانس}
جدول \ref{tab:performance_95} عملکرد مدل‌ها را با حفظ 95\% واریانس نشان می‌دهد:
\begin{latin}
\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
دیتاست & نوع داده & مدل & دقت (\%) & زمان آموزش (ثانیه) \\
\midrule
\lr{Wine} & اصلی & \lr{KNN} & 97.22 & $\sim$0 \\
\lr{Wine} & \lr{SVD} & \lr{KNN} & 97.22 & $\sim$0 \\
\lr{Wine} & \lr{EVD} & \lr{KNN} & 97.22 & $\sim$0 \\
\lr{Sensorless} & اصلی & \lr{NN} & 99.00 & 8.23 \\
\lr{Sensorless} & \lr{SVD} & \lr{NN} & 98.07 & 7.92 \\
\lr{Sensorless} & \lr{EVD} & \lr{NN} & 98.36 & 8.47 \\
\lr{MNIST} & اصلی & \lr{NN} & 97.70 & 27.82 \\
\lr{MNIST} & \lr{SVD} & \lr{NN} & 97.06 & 18.14 \\
\lr{MNIST} & \lr{EVD} & \lr{NN} & 97.46 & 15.31 \\
\lr{20 Newsgroups} & اصلی & \lr{NN} & 59.01 & 8.38 \\
\lr{20 Newsgroups} & \lr{SVD} & \lr{NN} & 55.14 & 5.94 \\
\lr{20 Newsgroups} & \lr{EVD} & \lr{NN} & 55.43 & 5.63 \\
\lr{Covertype} & اصلی & \lr{NN} & 86.61 & 36.50 \\
\lr{Covertype} & \lr{SVD} & \lr{NN} & 83.10 & 35.37 \\
\lr{Covertype} & \lr{EVD} & \lr{NN} & 83.27 & 36.89 \\
\bottomrule
\end{tabular}
\caption{عملکرد مدل‌ها با حفظ 95\% واریانس}
\label{tab:performance_95}
\end{table}
\end{latin}

\subsection{حفظ 85\% واریانس}
جدول \ref{tab:performance_85} عملکرد مدل‌ها را با حفظ 85\% واریانس نشان می‌دهد. توجه: زمان‌های آموزش ممکن است به دلیل شرایط متفاوت اجرا با نتایج 95\% قابل مقایسه نباشند.

\begin{latin}
\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
دیتاست & نوع داده & مدل & دقت (\%) & زمان آموزش (ثانیه) \\
\midrule
\lr{Wine} & اصلی & \lr{KNN} & 97.22 & $\sim$0 \\
\lr{Wine} & \lr{SVD} & \lr{KNN} & 97.22 & $\sim$0 \\
\lr{Wine} & \lr{EVD} & \lr{KNN} & 97.22 & $\sim$0 \\
\lr{Sensorless} & اصلی & \lr{NN} & 98.74 & 4.39 \\
\lr{Sensorless} & \lr{SVD} & \lr{NN} & 91.00 & 4.23 \\
\lr{Sensorless} & \lr{EVD} & \lr{NN} & 90.33 & 4.34 \\
\lr{MNIST} & اصلی & \lr{NN} & 97.45 & 9.77 \\
\lr{MNIST} & \lr{SVD} & \lr{NN} & 97.20 & 6.82 \\
\lr{MNIST} & \lr{EVD} & \lr{NN} & 96.90 & 6.72 \\
\lr{20 Newsgroups} & اصلی & \lr{NN} & 58.17 & 4.49 \\
\lr{20 Newsgroups} & \lr{SVD} & \lr{NN} & 56.72 & 2.38 \\
\lr{20 Newsgroups} & \lr{EVD} & \lr{NN} & 56.66 & 2.22 \\
\lr{Covertype} & اصلی & \lr{NN} & 87.43 & 22.52 \\
\lr{Covertype} & \lr{SVD} & \lr{NN} & 82.42 & 21.39 \\
\lr{Covertype} & \lr{EVD} & \lr{NN} & 82.62 & 21.92 \\
\bottomrule
\end{tabular}
\caption{عملکرد مدل‌ها با حفظ 85\% واریانس}
\label{tab:performance_85}
\end{table}
\end{latin}

% فصل 4: بحث
\chapter{بحث}
نتایج نشان می‌دهند که کاهش ابعاد با \lr{SVD} و \lr{EVD} تعداد ویژگی‌ها را به طور قابل توجهی کاهش داد، در حالی که \lr{95\%} واریانس حفظ شد. برای مثال، در دیتاست \lr{MNIST}، تعداد ویژگی‌ها از \lr{784} به \lr{331} کاهش یافت، و در \lr{20 Newsgroups} از \lr{5000} به حدود \lr{3172} (\lr{SVD}) و \lr{3183} (\lr{EVD}). با کاهش واریانس به \lr{85\%}، تعداد ویژگی‌ها بیشتر کاهش یافت (مثلاً \lr{MNIST} به \lr{185} و \lr{20 Newsgroups} به حدود \lr{2121}/\lr{2136})، اما افت دقت نیز افزایش یافت.

از نظر عملکرد مدل‌ها:



    1 \textbf{\lr{Wine}}: دقت در داده‌های اصلی و کاهش‌یافته (\lr{95\%} و \lr{85\%}) یکسان (\lr{97.22\%}) بود، که نشان‌دهنده حفظ کامل اطلاعات کلیدی در این دیتاست کم‌بعد است.

    
    2 \textbf{\lr{Sensorless}}: با \lr{95\%} واریانس، دقت از \lr{99.00\%} به \lr{98.07\%} (\lr{SVD}) و \lr{98.36\%} (\lr{EVD}) کاهش یافت، با بهبود جزئی در زمان آموزش. با \lr{85\%} واریانس، دقت به \lr{91.00\%} (\lr{SVD}) و \lr{90.33\%} (\lr{EVD}) افت کرد، که نشان‌دهنده از دست دادن اطلاعات مهم‌تر است.

    
    3 \textbf{\lr{MNIST}}: با \lr{95\%} واریانس، دقت از \lr{97.70\%} به \lr{97.06\%} (\lr{SVD}) و \lr{97.46\%} (\lr{EVD}) کاهش یافت، اما زمان آموزش از \lr{27.82} ثانیه به \lr{18.14} ثانیه (\lr{SVD}) و \lr{15.31} ثانیه (\lr{EVD}) بهبود یافت. با \lr{85\%} واریانس، دقت به \lr{97.20\%} (\lr{SVD}) و \lr{96.90\%} (\lr{EVD}) کاهش یافت، با زمان آموزش کمتر (\lr{6.82} ثانیه و \lr{6.72} ثانیه).

    
    4 \textbf{\lr{20 Newsgroups}}: با \lr{95\%} واریانس، دقت از \lr{59.01\%} به \lr{55.14\%} (\lr{SVD}) و \lr{55.43\%} (\lr{EVD}) کاهش یافت، با بهبود زمان آموزش از \lr{8.38} ثانیه به حدود \lr{5.94} و \lr{5.63} ثانیه. با \lr{85\%} واریانس، دقت به \lr{56.72\%} (\lr{SVD}) و \lr{56.66\%} (\lr{EVD}) کاهش یافت، با زمان آموزش کمتر (\lr{2.38} ثانیه و \lr{2.22} ثانیه).

    
    5 \textbf{\lr{Covertype}}: با \lr{95\%} واریانس، دقت از \lr{86.61\%} به \lr{83.10\%} (\lr{SVD}) و \lr{83.27\%} (\lr{EVD}) کاهش یافت، با زمان آموزش تقریباً ثابت. با \lr{85\%} واریانس، دقت به \lr{82.42\%} (\lr{SVD}) و \lr{82.62\%} (\lr{EVD}) کاهش یافت.

مقایسه \lr{SVD} و \lr{EVD} نشان می‌دهد که هر دو روش عملکرد مشابهی دارند، اما \lr{EVD} در برخی موارد (مانند \lr{MNIST} و \lr{20 Newsgroups}) دقت کمی بالاتری داشت. این تفاوت‌ها ناچیز هستند و هر دو روش برای کاهش ابعاد مناسب‌اند. کاهش ابعاد در دیتاست‌های با ابعاد بالا مانند \lr{MNIST} و \lr{20 Newsgroups} بسیار موثر بود، زیرا زمان آموزش را به طور قابل توجهی کاهش داد. با این حال، در دیتاست‌های متنی مانند \lr{20 Newsgroups}، افت دقت بیشتر بود، که نشان‌دهنده اهمیت اطلاعات جزئی‌تر در این نوع داده‌هاست.

در مورد مصرف منابع، زمان آموزش به‌عنوان معیاری برای کارایی محاسباتی استفاده شد. کاهش ابعاد به طور کلی مصرف منابع را کاهش داد، به‌ویژه در دیتاست‌های پرابعاد. با این حال، در دیتاست‌های با ابعاد متوسط مانند \lr{Sensorless} و \lr{Covertype}، بهبود زمان آموزش کمتر بود.

% فصل 5: نتیجه‌گیری
\chapter{نتیجه‌گیری}
این پروژه نشان داد که روش‌های \lr{SVD} و \lr{EVD} می‌توانند ابعاد داده‌ها را به طور موثری کاهش دهند و زمان آموزش مدل‌های یادگیری ماشین را بهبود بخشند، در حالی که دقت در اکثر موارد تنها کمی کاهش می‌یابد. این روش‌ها به‌ویژه برای دیتاست‌های با ابعاد بالا مانند \lr{MNIST} و \lr{20 Newsgroups} مفید هستند. در دیتاست‌های کم‌بعد مانند \lr{Wine}، کاهش ابعاد تاثیر چندانی بر دقت نداشت، اما در دیتاست‌های پیچیده‌تر مانند \lr{20 Newsgroups}، افت دقت قابل توجه بود. آزمایش‌های با \lr{85\%} واریانس نشان دادند که کاهش بیشتر واریانس منجر به افت دقت بیشتری می‌شود، که اهمیت حفظ واریانس بالاتر را نشان می‌دهد.

برای آینده، پیشنهاد می‌شود:


    * آزمایش با درصدهای مختلف واریانس (مانند \lr{90\%} یا \lr{99\%}) برای بررسی تعادل بین دقت و کارایی.

    
    * استفاده از مدل‌های یادگیری ماشین پیشرفته‌تر مانند شبکه‌های عمیق‌تر یا الگوریتم‌های دیگر.

    
    * بررسی روش‌های کاهش ابعاد دیگر مانند \lr{PCA} یا \lr{t\ndash SNE}.

% منابع
\begin{thebibliography}{9}
\bibitem{bishop}
\begin{latin}

Bishop, C. M. (2006). \textit{Pattern Recognition and Machine Learning}. Springer.
\bibitem{strang}
Strang, G. (2016). \textit{Linear Algebra and Its Applications}. Cengage Learning.

\href{https://www.latex-project.org/}{Project Git Hub}
\end{latin}
\end{thebibliography}

\end{document}
